# Visual Cue Effects on a Classification Accuracy Estimation Task in Immersive Scatterplots

Supplementary Materials

**Abstract**---Immersive visualization in virtual reality (VR) allows us to exploit visual cues for perception in 3D space, yet few existing studies have measured the effects of visual cues. Across a desktop monitor and a head-mounted display (HMD), we assessed scatterplot designs which vary their use of visual cues—motion, shading, perspective (graphical projection), and dimensionality—on two sets of data. We conducted a user study with a summary task in which 32 participants estimated the classification accuracy of an artificial neural network from the scatterplots. With Bayesian multilevel modeling, we capture the intricate visual effects and find that no cue alone explains all the variance in estimation error. Visual motion cues generally reduce participants’ estimation error; besides this motion, using other cues may increase participants’ estimation error. Using an HMD, adding visual motion cues, providing a third data dimension, or showing a more complicated dataset leads to longer response times. We speculate that most visual cues may not strongly affect perception in immersive analytics unless they change people’s mental model about data. In summary, by studying participants as they interpret the output from a complicated machine learning model, we advance our understanding of how to use the visual cues in immersive analytics.

## Authors
   - Fumeng Yang, James Tompkin, Lane Harrison, and David H. Laidlaw
   
## Visual-Cues-in-VR.mp4
   - a short video to illustrate the experimental conditions

## literature review.pdf
   - our literature review document
   
## data
   - `metas.csv` : participants' answers to the open-ended questions and demographics information
      -  **familiarML**: self-rated familiarity with machine learning
      -  **familiarDR**: self-rated familiarity with dimension reduction techniques
      -  **familiarVIS**: self-rated familiarity with data visualization
      -  **familiarVR**: self-rated familiarity with virtual reality
      -  **Session{1,2,3,4}Q{1,2}**: participants answers to the two open-ended questions at the end of each estimation session
      -  **PostDataFamilarity**: the answer to the post-experiment question: familiarity with the datasets
      -  **PostComments**: free comments
      -  **PostGlasses**: the answer to the post-experiment question: if pariticpants used glasses, etc.
      - **PostSickness**: the answer to the post-experiment question: if pariticpants felt sick in the experiment.
      -  **the remaining columns**: time stamps

   - `trials.csv`
      -  **SessionIndex**: which estimation session
      -  **SessionStart**,**SessionEnd**,**PracticeStart**,**PracticeEnd**,**MainStart**,**MainEnd**,**QuestionStart**,**QuestionEnd**: these are timestamps
      -  **TrialStatus**: main or practice
      -  **TrialIndex**
      -  **TrialStart**,**TrialVisualizationStart**,**TrialVisualizationEnd**,**TrialAnswerStart**,**TrialAnswerEnd**,**TrialFeedbackStart**,**TrialEnd**: timestamps for each trial, the response time is the difference between **TrialAnswerStart** and **TrialVisualizationStart**
      -  **TrialAnswer**: participants' answer to this trial
      -  **TrialIntAccuracy**: the true accuracy, rounded 
      -  **TrialSolution**: the true accuracy in decimal numbers
      -  **TrialDataset**: cifar-10 or babi, which data model
      -  **FactorStereo**,**FactorMotion**,**FactorPerspective**,**FactorShading**,**Dim**: each cue
      -  **Step**,**BinIndex**,**BinLower**,**BinUpper**,**File**: the refering to which file to use

## analysis
   - the analysis script/RMarkdown file

## python
#### generating participants
   - the script to randomize experimental variables and generate participants
   
#### machine learning
   - the scripts to train the two neural networks (they may take a few hours to run)
   - the scripts to compute t-SNE projections (they may take a day to run)
   - all saved intermediate neural networks (checkpoints)
   - the images of the projections generated by Python to check convergence

## experiment
   - the experiment system code based on Qt, OpenGL, SteamVR, etc.
   

